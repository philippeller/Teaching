{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10., 10.]\n",
    "plt.rcParams['xtick.labelsize'] = 14\n",
    "plt.rcParams['ytick.labelsize'] = 14 \n",
    "plt.rcParams['axes.labelsize'] = 16\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['legend.fontsize'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "This notebook demonstrates various deep learning architectures using the MNIST data\n",
    "\n",
    "The code uses Tensorflow / Keras, which you may need to install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "y = np.int32(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:60000]\n",
    "X_test = X[60000:]\n",
    "y_train = y[:60000]\n",
    "y_test = y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing so-called \"one hot\" encoding on the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat_test = keras.utils.to_categorical(y_test)\n",
    "y_cat_train = keras.utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are multiple architecture examples. Try out different ones and build your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'simple'\n",
    "\n",
    "# inputs = keras.Input(shape=(X_train.shape[1],))\n",
    "# h = keras.layers.Dense(128, activation=\"relu\")(inputs)\n",
    "# h = keras.layers.Dense(64, activation=\"relu\")(h)\n",
    "# h = keras.layers.Dense(32, activation=\"relu\")(h)\n",
    "# outputs = keras.layers.Dense(10, activation='softmax')(h)\n",
    "\n",
    "# models[name] = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# optimizer = keras.optimizers.Adam(0.0001)\n",
    "\n",
    "# models[name].compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "# models[name].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'dropout'\n",
    "\n",
    "# inputs = keras.Input(shape=(X_train.shape[1],))\n",
    "# h = keras.layers.Dropout(0.01)(inputs)\n",
    "# h = keras.layers.Dense(128, activation=\"relu\")(h)\n",
    "# h = keras.layers.Dropout(0.01)(h)\n",
    "# h = keras.layers.Dense(64, activation=\"relu\")(h)\n",
    "# h = keras.layers.Dropout(0.01)(h)\n",
    "# h = keras.layers.Dense(32, activation=\"relu\")(h)\n",
    "# outputs = keras.layers.Dense(10, activation='softmax')(h)\n",
    "\n",
    "# models[name] = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# optimizer = keras.optimizers.Adam(0.0001)\n",
    "\n",
    "# models[name].compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'batchnorm'\n",
    "\n",
    "\n",
    "# inputs = keras.Input(shape=(X_train.shape[1],))\n",
    "# h = keras.layers.Dense(128, activation=\"relu\")(inputs)\n",
    "# h = keras.layers.BatchNormalization()(h)\n",
    "# h = keras.layers.Dense(64, activation=\"relu\")(h)\n",
    "# h = keras.layers.BatchNormalization()(h)\n",
    "# h = keras.layers.Dense(32, activation=\"relu\")(h)\n",
    "# outputs = keras.layers.Dense(10, activation='softmax')(h)\n",
    "\n",
    "# models[name] = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# optimizer = keras.optimizers.Adam(0.0001)\n",
    "\n",
    "# models[name].compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'locally connected'\n",
    "\n",
    "\n",
    "# inputs = keras.Input(shape=(28, 28, 1))\n",
    "# h = keras.layers.LocallyConnected2D(1,  kernel_size=(5, 5), activation=\"relu\")(inputs)\n",
    "# h = keras.layers.LocallyConnected2D(1,  kernel_size=(5, 5), activation=\"relu\")(h)\n",
    "# h = keras.layers.Flatten()(h)\n",
    "# h = keras.layers.Dense(32, activation=\"relu\")(h)\n",
    "# h = keras.layers.Dense(16, activation=\"relu\")(h)\n",
    "# outputs = keras.layers.Dense(10, activation='softmax')(h)\n",
    "\n",
    "# models[name] = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# optimizer = keras.optimizers.Adam(0.0001)\n",
    "\n",
    "# models[name].compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'cnn_simple'\n",
    "\n",
    "\n",
    "# inputs = keras.Input(shape=(28, 28, 1))\n",
    "# h = keras.layers.Conv2D(1,  kernel_size=(5, 5), activation=\"relu\")(inputs)\n",
    "# h = keras.layers.Conv2D(1,  kernel_size=(5, 5), activation=\"relu\")(h)\n",
    "# h = keras.layers.Flatten()(h)\n",
    "# h = keras.layers.Dense(32, activation=\"relu\")(h)\n",
    "# h = keras.layers.Dense(16, activation=\"relu\")(h)\n",
    "# outputs = keras.layers.Dense(10, activation='softmax')(h)\n",
    "\n",
    "# models[name] = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# optimizer = keras.optimizers.Adam(0.0001)\n",
    "\n",
    "# models[name].compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'CNN'\n",
    "\n",
    "inputs = keras.Input(shape=(28, 28, 1))\n",
    "h = keras.layers.Conv2D(32,  kernel_size=(3, 3), activation=\"relu\")(inputs)\n",
    "h = keras.layers.MaxPool2D(pool_size=(2,2))(h)\n",
    "h = keras.layers.Conv2D(64,  kernel_size=(3, 3), activation=\"relu\")(h)\n",
    "h = keras.layers.MaxPool2D(pool_size=(2,2))(h)\n",
    "h = keras.layers.Conv2D(64,  kernel_size=(3, 3), activation=\"relu\")(h)\n",
    "h = keras.layers.Flatten()(h)\n",
    "h = keras.layers.Dense(16, activation=\"relu\")(h)\n",
    "outputs = keras.layers.Dense(10, activation='softmax')(h)\n",
    "\n",
    "models[name] = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(0.0001)\n",
    "\n",
    "models[name].compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models['CNN'].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'CNN + Dropout + Batchnorm'\n",
    "\n",
    "\n",
    "# inputs = keras.Input(shape=(28, 28, 1))\n",
    "# h = keras.layers.Dropout(0.01)(inputs)\n",
    "# h = keras.layers.Conv2D(32,  kernel_size=(3, 3), activation=\"relu\")(h)\n",
    "# h = keras.layers.BatchNormalization()(h)\n",
    "# h = keras.layers.MaxPool2D(pool_size=(2,2))(h)\n",
    "# h = keras.layers.Conv2D(64,  kernel_size=(3, 3), activation=\"relu\")(h)\n",
    "# h = keras.layers.BatchNormalization()(h)\n",
    "# h = keras.layers.MaxPool2D(pool_size=(2,2))(h)\n",
    "# h = keras.layers.Conv2D(64,  kernel_size=(3, 3), activation=\"relu\")(h)\n",
    "# h = keras.layers.BatchNormalization()(h)\n",
    "# h = keras.layers.Flatten()(h)\n",
    "# h = keras.layers.Dense(16, activation=\"relu\")(h)\n",
    "# outputs = keras.layers.Dense(10, activation='softmax')(h)\n",
    "\n",
    "# models[name] = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# optimizer = keras.optimizers.Adam(0.0001)\n",
    "\n",
    "# models[name].compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# models[name].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model using several epochs (1 epoch = churning through the full dataset once)\n",
    "\n",
    "NB: depending on the model, you need to shape the inputs differently!\n",
    "\n",
    "Training 30 Epochs (depending on the model and your computer hardware) can take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 1d models (dense etc)\n",
    "# models[name].fit(X_train, y_cat_train, epochs=30, validation_data=(X_test, y_cat_test), batch_size=64)\n",
    "\n",
    "# traind 2d models (CNNs etc)\n",
    "models[name].fit(X_train.reshape(-1, 28, 28, 1), y_cat_train, epochs=30, validation_data=(X_test.reshape(-1, 28, 28, 1), y_cat_test), batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the training history can help gaining some insight and sport overfitting for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in models.keys(): #['simple', 'CNN + Dropout + Batchnorm']: #'dropout', 'batchnorm']:\n",
    "    #bl = plt.plot(models[name].history.history['accuracy'], ls='--', label='Training Accuracy %s'%name)\n",
    "    #plt.plot(models[name].history.history['val_accuracy'], ls='-', c=bl[0].get_color(), label='Testing Accuracy %s'%name)\n",
    "    try:\n",
    "        bl = plt.plot(models[name].history.history['loss'], ls='--', label='Training Loss %s'%name)\n",
    "        plt.plot(models[name].history.history['val_loss'], ls='-', c=bl[0].get_color(), label='Testing Loss %s'%name)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "plt.gca().set_xlabel('Epoch')\n",
    "plt.gca().set_ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.gca().set_yscale('log')\n",
    "#plt.savefig('NN_history_cnn_best.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict 1d model\n",
    "#y_pred = models[name].predict(X_test)\n",
    "\n",
    "# predict 2d model\n",
    "y_pred = models[name].predict(X_test.reshape(-1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows how good the assignement of digits to the rerspective classis is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, np.argmax(y_pred,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cm.T, cmap='YlGnBu', origin='lower')\n",
    "plt.gca().set_xlabel('True label')\n",
    "plt.gca().set_ylabel('Predicted label')\n",
    "plt.savefig('NN_consfusion_%s.png'%name, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Try out different models and architectures and compare them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto encoder\n",
    "\n",
    "As discussed in the lecture, a different application of NNs are auto encoders.\n",
    "We first look at a linear auto encoder, which just replicates our good old PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear\n",
    "\n",
    "inputs = keras.Input(shape=(X_train.shape[1],))\n",
    "h = keras.layers.Dense(20, activation=\"linear\", use_bias=False)(inputs)\n",
    "outputs = keras.layers.Dense(X_train.shape[1], activation='linear', use_bias=False)(h)\n",
    "\n",
    "ae = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(0.00001)\n",
    "\n",
    "ae.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.fit(X, X, epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = keras.Model(inputs=inputs, outputs=h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = encode(X).numpy()\n",
    "\n",
    "plt_data = plt.scatter(reduced_data[:, 0], reduced_data[:, 1], s=0.1, c=y ,cmap=plt.cm.get_cmap('Spectral', 10))\n",
    "plt.colorbar()\n",
    "#plt.savefig('mnist_encoded_true_labels.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how our 20d recosntrcuted data looks like....pretty similar to our 20d PCA!\n",
    "Exercise: compare this NN to PCA in 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reco = ae(X).numpy()\n",
    "\n",
    "fig, ax = plt.subplots(5,5)\n",
    "for i in range(25):\n",
    "    axis = ax[i//5, i%5]\n",
    "    axis.imshow(X_reco[i].reshape(28,28), cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linear AE\n",
    "\n",
    "It gets much more powerful when adding back in non-linearirties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(X_train.shape[1],))\n",
    "encoded = keras.layers.Dense(256, activation=\"relu\")(inputs)\n",
    "encoded = keras.layers.Dense(64, activation=\"relu\")(encoded)\n",
    "encoded = keras.layers.Dense(2, activation=\"relu\")(encoded)\n",
    "decoder1 = keras.layers.Dense(64, activation=\"relu\")\n",
    "decoded = decoder1(encoded)\n",
    "decoder2 = keras.layers.Dense(256, activation=\"relu\")\n",
    "decoded = decoder2(decoded)\n",
    "decoder_out = keras.layers.Dense(X_train.shape[1], activation='linear')\n",
    "outputs = decoder_out(decoded)\n",
    "\n",
    "ae = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(0.001)\n",
    "\n",
    "ae.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.fit(X_train, X_train, epochs=30, validation_data=(X_test, X_test), batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can split up our models intwo the encoder and the decoder part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = keras.Model(inputs=inputs, outputs=encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_inp = keras.Input(shape=2,)\n",
    "decoded_i = decoder1(dec_inp)\n",
    "decoded_i = decoder2(decoded_i)\n",
    "outputs_i = decoder_out(decoded_i)\n",
    "decode = keras.Model(inputs=dec_inp, outputs=outputs_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = encode(X).numpy()\n",
    "reduced_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this 2d encoder, the digits separate much more nicely than in the PCA case, and also recosntrcuted images look fantastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_data = plt.scatter(reduced_data[:, 0], reduced_data[:, 1], s=0.1, c=y ,cmap=plt.cm.get_cmap('Spectral', 10))\n",
    "plt.colorbar()\n",
    "#plt.savefig('mnist_encoded_linear_true_labels.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reco = ae(X)\n",
    "X_plot = X_reco.numpy()\n",
    "\n",
    "fig, ax = plt.subplots(5,5)\n",
    "for i in range(25):\n",
    "    axis = ax[i//5, i%5]\n",
    "    axis.imshow(X_plot[i].reshape(28,28), cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate digits\n",
    "\n",
    "We can try to use the decoder as a generator, and generate artidicial digits. The issue here is that this may not work very well (see lecture) and should be done via _variational_ AEs (see according notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = np.array([[100., 100.],]).astype(np.float32)\n",
    "o = decode(inp).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a 2D manifold of the digits\n",
    "n = 15 # figure with 15x15 digits\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "\n",
    "grid_x = np.linspace(-100., 1600, n)\n",
    "grid_y = np.linspace(-100., 1200, n)\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]]).astype(np.float32)\n",
    "        x_decoded = decode.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(figure, cmap='Greys')\n",
    "\n",
    "plt.gca().get_xaxis().set_visible(False)\n",
    "plt.gca().get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "#plt.savefig('AE_mnist.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
