{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [12., 8.]\n",
    "plt.rcParams['xtick.labelsize'] = 14\n",
    "plt.rcParams['ytick.labelsize'] = 14 \n",
    "plt.rcParams['axes.labelsize'] = 16\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['legend.fontsize'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Algorithms\n",
    "\n",
    "In this notebook we will implement a few simple optimization algorithms, and explore more from the scipy.opt package on the 2d Rosenbrock test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rosenbrock test function in 2d\n",
    "\n",
    "The function in 2d is usually $f(x, y) = (a-x)^2 + b(y-x^2)^2$. Scipy provides this as a convenient function already, so we're going to use it and see how it looks like. It's minimum is at (1,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1.5,2,300)\n",
    "y = np.linspace(-2,3,300)\n",
    "\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "\n",
    "f = opt.rosen([xx, yy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(points=None, title=None):\n",
    "    pc = plt.pcolormesh(x, y, f, cmap='gist_heat_r', shading=\"nearest\")\n",
    "    plt.contour(x, y, f, levels=np.logspace(-1,4,10), colors='k', alpha=0.5)\n",
    "    plt.colorbar(pc)\n",
    "    plt.axvline(1, c='r', ls=':')\n",
    "    plt.axhline(1, c='r', ls=':')\n",
    "    if points is not None:\n",
    "        plt.plot(points[:,0], points[:,1], '.-',  c='k', lw=2)\n",
    "    if title is not None:\n",
    "        plt.gca().set_title(title)\n",
    "plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Gradient descent\n",
    "\n",
    "We start with the naive gradient descent algorithm as discussed in the lecture. Try out different alphas and see how it behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_gd(f, g, x0, alpha=0.001, iters=1000):\n",
    "    '''Naive gradient descent'''\n",
    "    x = [x0]\n",
    "    for i in range(iters):\n",
    "        grad = g(x[-1])\n",
    "        x.append(x[-1] - alpha*grad)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = naive_gd(opt.rosen, opt.rosen_der, x0 = np.array([-1, -1]), alpha=0.001, iters=100)\n",
    "plot(np.array(points), title='Simple GD, alpha=0.001, Iterations = 100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton Method\n",
    "\n",
    "Using second derivatives, we can implement the Newton method, that then converges very fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(f, g, h, x0):\n",
    "    '''Newtown method'''\n",
    "    x = [x0]\n",
    "    for i in range(5):\n",
    "        grad = g(x[-1])\n",
    "        hess = h(x[-1])\n",
    "        inv_hess = np.linalg.inv(hess)\n",
    "        x.append(x[-1] - np.dot(inv_hess, grad))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = newton(opt.rosen, opt.rosen_der, opt.rosen_hess, x0 = np.array([-1, -1]))\n",
    "plot(np.array(points), title='Newton')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nelder-Mead Simplex\n",
    "\n",
    "Implement the Nelder-Mead Simplex algorithm yourself as an exercise and see how it performs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate Gradient Descent\n",
    "\n",
    "We can explore more optimization algorithms, that are provided via the scipy.opt package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [[-1, -1]]\n",
    "def callback(xk, *args, **kwargs):\n",
    "    points.append(xk)\n",
    "res = opt.minimize(opt.rosen, jac=opt.rosen_der, x0=points[0], method=\"CG\", callback=callback)\n",
    "plot(np.array(points), title='CGD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method='GA'\n",
    "points = []\n",
    "def r(x):\n",
    "    points.append(x)\n",
    "    return opt.rosen(x)\n",
    "res = opt.differential_evolution(r, bounds=[[-1.5, 2], [-2,3]]) #, callback=callback)\n",
    "plot(np.array(points), title='GA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Try out more algorithms and settings. Also, you could check out pother packages, such as NLopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
