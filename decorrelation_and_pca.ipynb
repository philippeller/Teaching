{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from scipy import stats\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10., 10.]\n",
    "plt.rcParams['xtick.labelsize'] = 14\n",
    "plt.rcParams['ytick.labelsize'] = 14 \n",
    "plt.rcParams['axes.labelsize'] = 16\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['legend.fontsize'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data de-correlation\n",
    "\n",
    "We first generate some test data in 2d tat are distributed according to a multivariate normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = np.array([[0.07647196, 0.23147416],[0.23147416, 0.98215036]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = stats.multivariate_normal([1,1], cov=cov).rvs(10000).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(X, *args, **kwargs):\n",
    "    plt.plot(X[0], X[1], '.', ms=1, *args, **kwargs)\n",
    "    plt.gca().axvline(0, c='grey')\n",
    "    plt.gca().axhline(0, c='grey')\n",
    "    plt.gca().set_xlim(-3,3)\n",
    "    plt.gca().set_ylim(-3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(X)\n",
    "#plt.savefig('X.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtraction of Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X - np.mean(X, axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(X)\n",
    "#plt.savefig('X_0.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this space the sample covariance becomes a very simple expressions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covX = (X @ X.T) / (X.shape[1] - 1) \n",
    "covX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvalue decomposition\n",
    "\n",
    "We can use scipy's eigenvalue solver to to get the eigenvalues and eigenvectors. Using those we can define our transform `W`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L, E = scipy.linalg.eig(covX)\n",
    "W = E.T\n",
    "Y = W @ X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tranformed sample `Y` is not nicely de-correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(Y)\n",
    "#plt.savefig('Y_eigen.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further whiten the sample by sacling with the eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 1/np.sqrt(L.real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_eigen = (W @ X) * s[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(Y_eigen)\n",
    "#plt.savefig('Y_eigen_white.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting covariance is indeed the identity (up to nummeric imprecisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(Y_eigen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cholesky\n",
    "\n",
    "Another way to achieve basically the same is via Cholesky decomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_covX = np.linalg.inv(covX)\n",
    "L = np.linalg.cholesky(inv_covX)\n",
    "W = L.T\n",
    "Y_cholesky = W @ X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(Y_cholesky)\n",
    "#plt.savefig('Y_cholesky.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(Y_cholesky)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD\n",
    "\n",
    "Or via Singular value Decomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, V = np.linalg.svd(X.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_svd = ((V @ X).T @ np.diag(1/s *np.sqrt(X.shape[1] -1))).T\n",
    "plot(Y_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(Y_eigen, c='k', label='Eigen')\n",
    "plot(Y_cholesky, c='b', label='Cholesky')\n",
    "plot(Y_svd, c='r', label='SVD')\n",
    "plt.legend()\n",
    "#plt.savefig('Y_comp.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "Let's use our SVD to reduce our 2d data to 1d. The component we want to \"zero out\" is the one with the small variance (the red one in the below plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec(vec1, color='b'): \n",
    "    array = np.array([[0, 0, vec1[0], vec1[1]]])\n",
    "    x, y, u, v = zip(*array)\n",
    "    ax = plt.gca()\n",
    "    ax.quiver(x, y, u, v, color=color, angles='xy', scale_units='xy',scale=1)\n",
    "\n",
    "plot(X)\n",
    "vec(V[0])\n",
    "vec(V[1], \"r\")\n",
    "#plt.savefig('X_SVD.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for convenience turn s into a matrix S\n",
    "\n",
    "S = np.zeros((U.shape[0], V.shape[0]))\n",
    "np.fill_diagonal(S, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this by the way is the sample covariance\n",
    "V @ (np.square(S) / (X.shape[1] - 1) @ V.T)[:2]\n",
    "\n",
    "# and this the recosntructed data X from the SVD\n",
    "Xrec = (U @ S @ V).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now only use the first component (here indexed with `[0]`), and we can see that we successfully \"removed\" the small variance. The reconstrcuted sample from the 1d PCA back in the original space now is still 1d, but has most of the information of the original sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca0 = (U @ S)[:, [0]] @ V[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(X_pca0.T)\n",
    "#plt.savefig('pca0', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST PCA\n",
    "\n",
    "Let's explore the MNSIT dataset and use PCA to reduce its dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Those are the wrong ones: from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "y = np.int32(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(X):\n",
    "    fig, ax = plt.subplots(5,5)\n",
    "    for i in range(25):\n",
    "        axis = ax[i//5, i%5]\n",
    "        axis.imshow(X[i].reshape(28,28), cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(X)\n",
    "#plt.savefig('mnist.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the PCA functionality provided by `sklearn`, but it is a nice exercise to do it by hand as above! (Be ware that the samples can be singular!)\n",
    "Here let's do 20 components (See what happens for other numbers!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PCA(n_components=20)\n",
    "p.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good measure of how much detail of the original sample is preserved is the \"explained variance\". here we see that it is arounf 65-70% using only 20 dimensions to reduce our 784d data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "ax.bar(np.arange(p.n_components)+1, np.cumsum(p.explained_variance_ratio_), width=1, label='cumulative')\n",
    "#plt.bar(np.arange(p.n_components)+1, p.explained_variance_ratio_, width=1, label='per component')\n",
    "\n",
    "ax.set_ylabel('Explained Variance (%)')\n",
    "ax.set_xlabel('PCA component')\n",
    "\n",
    "#plt.savefig('explained variance.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use our PCA to apply it to the data, and also recosntrcut it in the original space again. They are quite clearly readable despite only using 20d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = p.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_rec = p.inverse_transform(reduced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(digits_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the first two componetns are distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_data = plt.scatter(reduced_data[:, 0], reduced_data[:, 1], s=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De-correlation of MNSIT \"by hand\" example\n",
    "\n",
    "Just for completness sake, let's see how we could go about doing an eigenvalue decomposition of MNIST ourselevs.\n",
    "Since data is 784 dimensional, we will just look at the correlation matrix as an image (see below). The regular staructure can be explained by the images pixel ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covX = np.cov(X.T)\n",
    "plt.imshow(covX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, v = np.linalg.eig(covX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(v.real.T @ covX @ v.real)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is a bit hard to see, we zoom in to the upper left corner, and see that indeed it is diagonal, with descending size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((v.real.T @ covX @ v.real)[:30,:30])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
