{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10., 10.]\n",
    "plt.rcParams['xtick.labelsize'] = 14\n",
    "plt.rcParams['ytick.labelsize'] = 14 \n",
    "plt.rcParams['axes.labelsize'] = 16\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['legend.fontsize'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "We explore different classification algorithms on a binary problem using the MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "y = np.int32(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with our standard PCE to 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PCA(n_components=2)\n",
    "p.fit_transform(X)\n",
    "reduced_data = p.transform(X)\n",
    "digits_rec = p.inverse_transform(reduced_data) #+ mean\n",
    "\n",
    "plt_data = plt.scatter(reduced_data[:, 0], reduced_data[:, 1], s=0.1, cmap=plt.cm.get_cmap('Spectral', 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The large difference to, for example clustering, is that now we have the truth lables! Let's plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_data = plt.scatter(reduced_data[:, 0], reduced_data[:, 1], s=0.1, c=y ,cmap=plt.cm.get_cmap('Spectral', 10))\n",
    "plt.colorbar()\n",
    "#plt.savefig('mnist_true_labels.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "\n",
    "To reduce the problem, we start with binary classification, trying to distinguish the digits (1, 4, 5, 7, 9) from the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do binary classification first\n",
    "\n",
    "target = ((y == 1) | (y == 9) | (y == 5) | (y == 7) | (y == 4)).astype(np.float32)\n",
    "\n",
    "plt_data = plt.scatter(reduced_data[:, 0], reduced_data[:, 1], s=0.1, c=target ,cmap=plt.cm.get_cmap(\"winter\", 2))\n",
    "plt.colorbar()\n",
    "#plt.savefig('mnist_binary_labels.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We always split data into independent training and testing samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = reduced_data[:60000]\n",
    "X_test = reduced_data[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = target[:60000]\n",
    "y_test = target[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a classifier, try out different ones and different settings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=20, max_depth=7) #DecisionTreeClassifier(max_depth=4))\n",
    "name = \"BDT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "# clf = LinearDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "# clf = MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# clf = DecisionTreeClassifier(max_depth=10, )\n",
    "# name = 'max. depth = %s'%clf.max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "#clf = RandomForestRegressor(n_estimators=20, max_depth=7, criterion='mse') #DecisionTreeClassifier(max_depth=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this does the learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following produces predicted lables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_label = clf.predict(X_train)\n",
    "y_test_predicted_label = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this produces a probability output for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = clf.predict_proba(X_train)\n",
    "y_test_predicted = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the classifier outputs for the testing and training samples of both classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 1, 20)\n",
    "plt.hist(y_predicted[y_train==0][:,1], bins=bins, color=plt.cm.winter(0), label='Train (y=0)', alpha=0.5)\n",
    "plt.hist(y_predicted[y_train==1][:,1], bins=bins, color=plt.cm.winter(1000), label='Train (y=1)', alpha=0.5)\n",
    "plt.hist(y_test_predicted[y_test==0][:,1], bins=bins, color=plt.cm.winter(0), histtype='step', lw=2, label='Test (y=0)', hatch='77')\n",
    "plt.hist(y_test_predicted[y_test==1][:,1], bins=bins, color='green', histtype='step', lw=2, label='Test (y=0)')\n",
    "plt.axvline(0.5, color='red', label = 'Standard threshold = 0.5')\n",
    "plt.legend()\n",
    "#plt.savefig('DT_dist.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amd this is how the predicted lables look in our scattering plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_data = plt.scatter(X_train[:, 0], X_train[:, 1], s=0.1, c=y_predicted[:,1] ,cmap=\"winter\")\n",
    "plt.colorbar()\n",
    "#plt.savefig('DT_proba.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well is the classification dping? We can construct the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_test_predicted_label, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cm.T, cmap='YlGnBu', origin='lower')\n",
    "plt.gca().set_xlabel('True label')\n",
    "plt.gca().set_ylabel('Predicted label')\n",
    "plt.savefig('DT_confusion.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curves\n",
    "\n",
    "Further insight provides the ROC curve (as discussed in the lecture).\n",
    "\n",
    "As it can be a bit difficult to understand at first, I highly recommen for you to construct some ROC curves by hand! Here we use the built-in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_test = {}\n",
    "tpr_test = {}\n",
    "fpr_train = {}\n",
    "tpr_train = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_test[name], tpr_test[name], _ = roc_curve(y_test, y_test_predicted[:,1])\n",
    "fpr_train[name], tpr_train[name], _ = roc_curve(y_train, y_predicted[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tpr_test.keys():\n",
    "    if key == 'max. depth = 50': continue\n",
    "    bl = plt.plot(fpr_test[key], tpr_test[key], ls='-', label='%s test (AUC = %0.2f)' %(key,auc(fpr_test[key], tpr_test[key])))\n",
    "    plt.plot(fpr_train[key], tpr_train[key], ls='--', c=bl[0].get_color(), label='%s train (AUC = %0.2f)' %(key, auc(fpr_train[key], tpr_train[key])))\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.savefig('overtraining.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot some of the testing data that was preducted to be of class (1, 4, 5, 7, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_plot = X[60000:]\n",
    "\n",
    "mask = y_test_predicted_label == 1\n",
    "\n",
    "fig, ax = plt.subplots(5,5)\n",
    "for i in range(25):\n",
    "    axis = ax[i//5, i%5]\n",
    "    axis.imshow(X_plot[mask][i].reshape(28,28), cmap='Greys')\n",
    "    \n",
    "    \n",
    "plt.suptitle('Class: 1, 4, 5, 7, 9')\n",
    "#plt.suptitle('Class: 0, 2, 3, 6, 8')\n",
    "    \n",
    "#plt.savefig('mnist.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Implement a multi-label classification for all 10 digits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
