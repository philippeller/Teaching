{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Networks**\n",
    "\n",
    "    1. Try out an auto-diff library (e.g. tensorflow) to automatically calculate derivatives of some test functions\n",
    "\n",
    "Let's start by generating some mock data, and a few test functions. For simplicity, I'll choose test functions that are not singular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "def generate_mock_data(n):\n",
    "    return 8 #np.random.uniform(-10,10, n)\n",
    "\n",
    "def x_squared(x):\n",
    "    return x**2\n",
    "\n",
    "def x_squared_diff(x):\n",
    "    return 2*x\n",
    "\n",
    "data = generate_mock_data(n = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now let's try to take our mock data, turn it into Tensorflow Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "import tensorflow as tf;\n",
    "data_tensorflow = tf.Variable(data, dtype = float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the original data back by simply calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tensorflow.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ask Tensorflow to watch out for changes to the data by calling with \"tf.GradientTape() as tape: \". In that white space, we can manipulate the data as we like, and then afterwards ask for gradients. Let's try to run the test functions on the tensor and then ask for gradients, and check that they are indeed correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    # For the squared function\n",
    "    y_squared = x_squared(data_tensorflow)\n",
    "    y_squared_gradient = tape.gradient(y_squared, data_tensorflow)\n",
    "\n",
    "print(y_squared_gradient.numpy() == x_squared_diff(data_tensorflow.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. So it works! You can read more about the auto differentiation in tensorflow here: https://www.tensorflow.org/guide/autodiff\n",
    "\n",
    "Now, let's try the same, but in PyTorch instead. First we turn the data into a Pytorch Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data_torch = torch.tensor([data], dtype = torch.float, requires_grad = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we specify \"requires_grad = True\" in the call to torch.tensor, we ask PyTorch to keep a track of the gradients of data_torch. Conceptually, it's identical to what we did above for tensorflow using \"tf.GradientTape()\". Let's now call \"x_squared\" on \"data_torch\" and compare gradients from torch with our analytical one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "y_squared = x_squared(data_torch)\n",
    "y_squared.backward(retain_graph = True) # calculates gradients via back-propagations\n",
    "print(data_torch.grad.item() == x_squared_diff(data_torch.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! So it works! You can read more about auto diff in pytorch here: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. Use This in your simple bisection search from exercise 3.1\n",
    "\n",
    "Alright. Let's wrap the auto diff code above into functions first. A few changes are made to the torch backend to make it accomodate arrays instead of scalars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def evaluate_function_and_get_gradients(f, data, backend = 'torch'):\n",
    "    if backend == 'torch':\n",
    "        data_torch = torch.tensor(data, dtype = torch.float, requires_grad = True)\n",
    "        y = f(data_torch)\n",
    "        y.backward(retain_graph = True, gradient = torch.tensor(np.repeat(1.0, len(data_torch))))\n",
    "        gradient = data_torch.grad.detach().numpy()\n",
    "        y = y.detach().numpy()\n",
    "        del data_torch \n",
    "    elif backend == 'tensorflow':\n",
    "        with tf.GradientTape() as tape:\n",
    "            data_tensorflow = tf.Variable(data, dtype = float)\n",
    "            y = f(data_tensorflow)\n",
    "            gradient= tape.gradient(y, data_tensorflow)\n",
    "            del data_tensorflow\n",
    "            y = y.numpy()\n",
    "    else:\n",
    "        assert backend in ['tensorflow', 'torch'], f'Recieved backend {backend} but this is not supported.'\n",
    "    return y, gradient   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. Let's snatch the code from the old exercise and adapt it to use autodiff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Gradient to be 0 at x = 0 in 20 iterations using tensorflow\n",
      "Found Gradient to be 0 at x = 0 in 20 iterations using torch\n",
      "True 0-point at x= [0]\n"
     ]
    }
   ],
   "source": [
    "def bisection_gradient(interval, f, backend = 'torch'):\n",
    "    y, gradient = evaluate_function_and_get_gradients(f,interval, backend)\n",
    "    iterations = 1\n",
    "    stop = False\n",
    "    allowed_indices = np.arange(0, len(y),1)\n",
    "    middle_index = len(allowed_indices)//2\n",
    "    while (stop == False) and (y[middle_index] != 0): \n",
    "        if  y[middle_index] > np.min(y):\n",
    "            allowed_indices[middle_index:] = -1\n",
    "        else:\n",
    "            allowed_indices[:middle_index] = -1\n",
    "        middle_index = allowed_indices[allowed_indices!= -1][len(allowed_indices[allowed_indices!= -1])//2] \n",
    "        iterations += 1\n",
    "        if iterations > 500:\n",
    "            stop = True\n",
    "    return middle_index, y[middle_index], iterations\n",
    "\n",
    "interval = np.arange(0,1000000,1)\n",
    "backend = 'tensorflow'\n",
    "middle_index, lowest_point, iterations = bisection_gradient(interval, x_squared, backend= backend)\n",
    "print(f\"Found Gradient to be 0 at x = {middle_index} in {iterations} iterations using {backend}\")\n",
    "\n",
    "backend = 'torch'\n",
    "middle_index, lowest_point, iterations = bisection_gradient(interval, x_squared, backend= backend)\n",
    "print(f\"Found Gradient to be 0 at x = {middle_index} in {iterations} iterations using {backend}\")\n",
    "print(f\"True 0-point at x= {interval[x_squared_diff(interval)==0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49fa95f8682c3efc604b9952cbd156675841b5fb5062eb2aceaa794a0e20c2ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
